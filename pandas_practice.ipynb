{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41ae5639-bbb0-45a1-a8ab-55f85b607a94",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babbda89-d563-4b84-8f8d-556d5dd33c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e1dbd8-4cfd-44aa-9c82-a630b0dced51",
   "metadata": {},
   "source": [
    "## Importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f000b358-cf50-4e04-a1b1-794a3fa23cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import the dataset\n",
    "\n",
    "df = pd.read_csv(r'E:\\Study\\Projects\\EDA\\generated_dataset.csv')\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74214e9-34d2-46e6-8930-6559b462b087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's view the top 5 records of our dataset\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e15ced6-e217-4ade-b95a-5c515b6d9639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find the no. of rows and columns of our dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9febf7ca-3a7c-4769-a9af-9fd28819aca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the information or description of our dataset like column names, datatype, null or not etc.\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5ee06e-9ae2-43d2-aacf-af16e16dc2ae",
   "metadata": {},
   "source": [
    "## 1. Load csv file and display first 10 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bb3526-3506-4962-bc77-c822c258526a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we have already loaded the dataset, we will display the first 10 rows.\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0250317f-4380-43d2-a3ce-7f34b8a63fb4",
   "metadata": {},
   "source": [
    "## 2. Filter the rows in a DataFrame where the column sales is greater than 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5be8a53-d6d5-4216-9b9c-bda90c5370cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "greater_sales = df[df['sales'] > 1000]\n",
    "greater_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ddffe1-c34e-4878-bc8b-7805a0ff86de",
   "metadata": {},
   "source": [
    "## 3. Select only columns 'name' and 'sales'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37935598-5ed1-4b7a-b315-6d8d72ddd93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_and_sales = df[['name','sales']]\n",
    "name_and_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1731ffbb-da6f-4ab4-a872-cf1c0feeb4e0",
   "metadata": {},
   "source": [
    "## 4. Sort a DataFrame by the column date in ascending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bde7d2-589d-4c0e-9e1e-5e0f0cac2d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = sorted(df['date'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0881812-b9a1-4567-9b5b-a92c65aa146f",
   "metadata": {},
   "source": [
    "## 5. Add a new column called profit which is the difference between the columns revenue and cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e4775a-2ba6-496e-90e0-39015a325a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['profit'] = df['revenue'] - df['cost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1dc565-faf9-4d53-b5c9-e946bf893bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bf746c-45c1-4714-9b34-c3dbd758276e",
   "metadata": {},
   "source": [
    "## 6. Rename the column sales to total_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19794f91-c618-4709-834c-12e9c427357b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the column sales to total_sales\n",
    "\n",
    "df.rename(mapper={'sales':'total_sales'},axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523c4676-8df5-46e5-b245-45c4573b9934",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3f11d8-87cf-4563-9816-6daaf954adf2",
   "metadata": {},
   "source": [
    "## 7. Drop all rows with missing values in any column of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afab5b9-f685-4e6a-a97d-8f7add2bf010",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3035f63f-6332-4ef8-8dbb-add20d06d7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to check whether the dataframe is null or not.\n",
    "#If it is null, the specific column will give the output more than 0.\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da03e230-3ea1-44f9-816c-73413b284f84",
   "metadata": {},
   "source": [
    "## 8. Find the number of unique values in the column region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a9da4f-3f86-460c-ba1b-fce75d2c7340",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5abdb2-1471-47d6-a0c5-59df4a668743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is the code to find the no. of unique values in the region column\n",
    "df['region'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5f0721-7629-48cc-8c13-aba68d555665",
   "metadata": {},
   "source": [
    "## 9. Group the DataFrame by the column category and calculate the sum of sales for each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f943600-be99-45aa-91b6-d72024c83ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is the code to group the category and the sum of sales for each group.\n",
    "group_sales = df.groupby('category')['total_sales'].sum()\n",
    "group_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a82bd67-9f7b-48de-9a2b-8106b9e505cd",
   "metadata": {},
   "source": [
    "## 10. Create a DataFrame from a dictionary of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8833945-ebfc-4088-adfa-a61056145566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a data argument of DataFrame using dict and index using the split().\n",
    "dat = pd.DataFrame(\n",
    "    {\n",
    "    'S.No':list(range(1, 7)),\n",
    "    'Name':'Arun Kumar Shanmuga Priya Mithran Tharika'.split(),\n",
    "    'Age': [35, 30, 8, 3, 35, 30]\n",
    "    },index='Fa ther Mo ther Fkid Skid'.split()\n",
    ")\n",
    "dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cb388a-c452-415a-b8ab-efb67df25277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a dataframe using random numerical values of numpy library.\n",
    "# This is just another way of creation of dataframe.\n",
    "\n",
    "from numpy.random import randn\n",
    "data = pd.DataFrame(randn(40,9), list(range(0,40)), 'fan_id table_id board_id door_id cloth_id book_id bag_id doll_id light_id'.split())\n",
    "data.iloc[1, 8] = np.nan  # Missing value in row 1, column 8\n",
    "data.iloc[2, 1] = np.nan  # Missing value in row 2, column 1\n",
    "data.iloc[3, 6] = np.nan  # Missing value in row 3, column 6\n",
    "data.iloc[5, 3] = np.nan  # Missing value in row 5, column 3\n",
    "data.iloc[10, 6] = np.nan  # Missing value in row 10, column 6\n",
    "data.iloc[11:13, 4] = np.nan  # Missing value in row 10, column 6\n",
    "data.iloc[20:22, 1] = np.nan  # Missing values in rows 20-21, column 1\n",
    "data.iloc[30, [2, 4]] = np.nan  # Missing values in row 30, columns 2 and 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce1ed3f-3836-458e-8f95-22d3629a8ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0300605a-f35b-4aa5-96b9-1fba341b71ef",
   "metadata": {},
   "source": [
    "## 11. Check if there are any missing values in a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dcb535-79d1-4f21-900a-c53941c202a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check if there is any missing values in dataframe. We can do this in 2 ways.\n",
    "# First way which is straight forward.\n",
    "\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cee0e94-3348-4df8-ab8d-f5b79eb51dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the total missing values in a dataframe.\n",
    "len(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04503dcc-828b-4f75-a813-2ff10b3ae32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second method to check the null values of a dataframe using list comprehension\n",
    "nan_feature = [feature for feature in data.columns if data[feature].isnull().sum() > 1]\n",
    "for feature in nan_feature:\n",
    "    print(f'{feature}: {np.round(data[feature].isnull().mean(), 4)} % of null values')\n",
    "    #print(f'{feature} {np.round(df[feature]).isnull().mean(), 4} : % of features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbeaba64-fac5-48b8-8e5d-c418e0068c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1784a207-53c1-4edb-8250-98b0ff9077e5",
   "metadata": {},
   "source": [
    "## 12. Replace all occurrences of the value -1 in the column rating with NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22332e3b-5053-4491-b103-7b6b864ebd3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Since we don't have -1 in rating column we will replace few values listed below with -1\n",
    "# Note, we are using 'df' dataframe and not 'data' dataframe.\n",
    "df.replace([1.11, 1.58, 3.49, 4.05, 3.89, 1.27], [-1, -1, -1, -1, -1, -1], inplace=True)\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d22485-aa6c-44df-944b-c25c7204472c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now replace all -1 values with NaN\n",
    "df.replace(-1, np.NaN, inplace=True)\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336ab69c-fcc6-4b8b-ac7a-da39b3734be3",
   "metadata": {},
   "source": [
    "## 13. Filter rows where the column product starts with the letter A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f0c419-bdfc-4166-b10f-2ce211fbb387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we don't have any values that starts with A, we will first replace few values with A.\n",
    "df.replace('Product_3 Product_19 Product_12 Product_7 Product_2 Product_11 Product_16 Product_19'.split(),\n",
    "           'Arun arumugam rama senthalampoo shiva senathipathi Aravind Aran'.split(), inplace=True)\n",
    "#column_with_A = df['product'].filter(like = 'A')\n",
    "#column_with_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a14490-b62d-4911-831b-c47c80e6291a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the product column values that starts with A only\n",
    "# Remember since python is case sensitive, it ignored a column value that starts with small 'a'\n",
    "\n",
    "columns_with_A = df[df['product'].str.startswith('A')]\n",
    "columns_with_A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7fb824-b8cd-4fa1-abdb-589691bae184",
   "metadata": {},
   "source": [
    "## 14. Add a column called discounted_price by applying a 10% discount to the price column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4eec8c-6f2c-4285-9997-91bbcdb4e397",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['discounted_price'] = 0.01 * df['cost']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae22760c-800a-4e2c-9b40-e18b63b4abba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the position of columns. The column discounted_price & profit are in the last position.\n",
    "# Moving it to the 11 position\n",
    "#column = list(df.columns)\n",
    "#column.insert(11, column.pop(-1))\n",
    "#df.head()\n",
    "columns_to_move = ['profit', 'discounted_price']\n",
    "new_order = (list(df.columns[:10]) + columns_to_move + list(df.columns[10:-2]))\n",
    "df = df[new_order]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1ec057-45b1-4a6b-bcad-b7df31673c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f075237f-5593-4c75-88db-14edde1a1c23",
   "metadata": {},
   "source": [
    "## 15. Merge two DataFrames on a common column called id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dfdf87-3e46-4dad-92d7-ed21288bb46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.shape\n",
    "# Since we don't have id column in 'data' dataframe we first create it to merge with 'df' dataframe\n",
    "data['id'] = df['id']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5131d9-8478-47f0-99ba-fd61d24abe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc01a4df-f445-4e25-a841-101635f101f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the id column position\n",
    "column_name = list(data.columns)\n",
    "column_name.insert(0, column_name.pop(-1))\n",
    "data = data[column_name]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9b1e8d-5cc0-4fdf-ab0c-ac40b50d3dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the id column in 'data' dataframe is of int64 datatype, we change it to object datatype.\n",
    "# We do this because the id column of 'df' dataframe is object datatype.\n",
    "# Remember, to merge two dataframe object, it need to be same datatype.\n",
    "data['id'] = data['id'].astype('object')\n",
    "\n",
    "df = pd.merge(df, data, on='id', how = 'inner')\n",
    "df.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b612cc09-2076-4a92-8a0b-d62b1067297c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61216f65-5042-4d93-908e-3fec91b34abe",
   "metadata": {},
   "source": [
    "## 16. Create a DataFrame with only numerical columns from an existing DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4928b78-b28f-439d-8c1c-e45338cb24af",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = [feature for feature in df.columns if df[feature].dtypes != 'O']\n",
    "df[numerical_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5010e026-97e0-48e1-ac01-dd80d9449629",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108e4f3a-3f74-4d04-9670-bae667878cb8",
   "metadata": {},
   "source": [
    "## 17. Extract the year from a datetime column called order_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dece96-8088-454f-97c5-c02eb81ab415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we don't have any column called 'order_date' with datetime datatype, let's first create it\n",
    "# We now create a column called 'order_date' with datetime datatype.\n",
    "# We need to import datetime & timedelta method from datetime library.\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# We now create 'start_date' & 'end_date' variables to generate random dates between those dates\n",
    "start_date = datetime(2024, 1, 1)\n",
    "end_date = datetime(2024, 12, 31)\n",
    "random_dates = [start_date + timedelta(days=np.random.randint(0, (end_date - start_date).days))\n",
    "               for _ in range(22)\n",
    "               ]\n",
    "df['order_date'] = random_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a394b1b-2e0a-460c-9edf-01c9e141f423",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04dc3a2-2c7a-45cc-a2ee-25c7f9b04524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bringing the order_date column before rating column\n",
    "\n",
    "column_order = list(df.columns)\n",
    "column_order.insert(6, column_order.pop(-1))\n",
    "df = df[column_order]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b163e52-5213-4d4b-92d2-7ae58b8cd3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's extract the year alone with simple dt.year property in datetime datatype\n",
    "extracted_year = df['order_date'].dt.year\n",
    "extracted_year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bd41a6-c8f0-4a73-83e3-aa973d41e839",
   "metadata": {},
   "source": [
    "## 18. Save a DataFrame to a CSV file named output.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60473f5-7d09-4c07-81e7-8598a6bed378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save the dataframe in csv format using 'df' dataframe\n",
    "\n",
    "df.to_csv(r'E:\\Study\\Projects\\EDA\\output_pandas_practice.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f627978f-cb87-4e31-8b18-627a34151413",
   "metadata": {},
   "source": [
    "## 19. Reset the index of a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e3d732-76de-4100-8c7f-3a416f6555dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rst_indx = df.reset_index()\n",
    "rst_indx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d442d714-bf7e-4407-9dab-b9417579d633",
   "metadata": {},
   "source": [
    "## 20. Find the total sum of the column quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950ea542-967f-47e7-8834-0c73424b7c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_of_quantity = df['quantity'].sum()\n",
    "sum_of_quantity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0907ce54-0955-4c8c-8c9c-8b4de0868e7e",
   "metadata": {},
   "source": [
    "## 21. Extract rows where the rating column is between 3 and 4 (inclusive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec3a4bb-6ce1-4dfb-8b34-da4e63169653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use comparison operator here.\n",
    "# Remember python normal 'and' operator will throw error. we need to use '&' operator for more than one condition.\n",
    "# Also, note the parenthesis used for the seperation of two operations.\n",
    "\n",
    "rating_three_four = [feature for feature in (df['rating'] >= 3) & (df['rating'] <= 4)]\n",
    "df[rating_three_four]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e3bb8d-e57c-418b-96d5-48137d35e77e",
   "metadata": {},
   "source": [
    "## 22. Find the maximum value in the column profit grouped by region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7f5e8f-af5b-4187-8c10-97a01fcf02f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's group the maximum profit region wise\n",
    "df.groupby('region')['profit'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c0c3cb-45ba-4d6e-8617-948df1424ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the above maximum profit region wise in bar plot.\n",
    "\n",
    "df.groupby('region')['profit'].max().plot(kind='bar')\n",
    "plt.title('Region wise maximum profit')\n",
    "plt.xlabel('Region')\n",
    "plt.ylabel('Profits')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c997f1ca-e142-464f-95b1-bdbe6a7e2f2a",
   "metadata": {},
   "source": [
    "## 23. Create a pivot table with region as the index and sales as the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb4e75b-f9e5-427c-9686-636ab96984f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create the pivot table using the 'df' dataframe which is a mandatory argument.\n",
    "# The 'region' is used as an index and 'total_sales' is used as values argument.\n",
    "\n",
    "pivot_table = pd.pivot_table(data=df, values='total_sales', index='region')\n",
    "pivot_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7b3a46-bf25-4fe2-9c29-b201e5a3247c",
   "metadata": {},
   "source": [
    "## 24. Drop duplicate rows based on the product_id column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ea6d5a-833e-43f9-8492-a5b1876f7c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first check the columns having duplicate rows\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986710d1-567e-4e74-964d-6c046aed132c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since there is no duplicate rows in the above step, we are skipping the step of dropping the duplicates.\n",
    "# Since no duplicates, below code will not work.\n",
    "#df.drop_duplicates(inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657b812f-e769-49c0-bc61-826f8203e765",
   "metadata": {},
   "source": [
    "## 25. Concatenate two DataFrames vertically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c72273-e0e5-4fd1-89f1-7ed4e5c20a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can concatenate the dataframes 'dat', 'data'.\n",
    "# Note that the dataframes are passed in list as an argument.\n",
    "concatenated_df = pd.concat([dat, data])\n",
    "concatenated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b475db8-290b-4495-acf7-49d969757927",
   "metadata": {},
   "source": [
    "## 26. Replace all NaN values in a DataFrame with the mean of their respective columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c202bd6-0b83-450b-8f9c-12dfbfe05301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's bring all the NaN value features first using list comprehension.\n",
    "missing_features = [feature for feature in df.columns if df[feature].isnull().sum() > 0]\n",
    "missing_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b98cbf-560e-40eb-974c-b0543774cc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's replace the NaN values of all features with their mean values.\n",
    "for feature in missing_features:\n",
    "    df[feature] = df[feature].fillna(df[feature].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed619b8-91d1-47a0-a68f-ea8434672b0a",
   "metadata": {},
   "source": [
    "## 27. Find the average sales for each region and save the result as a new DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05a2b4f-6e2b-459f-9ab5-a1d7dc80f0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_sales_df = pd.DataFrame(df.groupby('region')['total_sales'].mean())\n",
    "average_sales_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dff6808-2995-4134-b37f-e68c89633ec6",
   "metadata": {},
   "source": [
    "## 28. Convert a column price from a string to a float data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1657c4-ba59-4691-ab17-057e7d3020d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the 'discounted_price' column is already in float64 dtype, we skip this process.\n",
    "# Perhaps, the below code works if the 'discounted_price' is of str datatype.\n",
    "#str_datatype = df['discounted_price'].astype(float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce1f078-65ab-4f43-b184-8c87d80f6de2",
   "metadata": {},
   "source": [
    "## 29. Extract all rows where the column status is either Shipped or Pending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5ac1b7-694c-4be6-bdca-807e6db0f899",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358b7052-405c-4dd4-a11e-97885331b192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since there is no status column we will first create a new column.\n",
    "# We create the new status column with 5 values shipped, pending, delivered, returned, ordered.\n",
    "status_values = ['shipped', 'pending', 'delivered', 'returned', 'ordered']\n",
    "df['status'] = np.random.choice(status_values, size=len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33868ac8-5219-460c-8626-094995bf8831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the status column position from last to 13\n",
    "status_position = list(df.columns)\n",
    "status_position.insert(13, status_position.pop(-1))\n",
    "df = df[status_position]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3853721-9f51-40cb-a15a-eb04ee574e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01665907-26db-4aa5-84db-168db8dc29dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_status = df[df['status'].isin(['shipped', 'pending'])]\n",
    "filtered_status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95459a0a-9829-4778-938c-b48053fca3cf",
   "metadata": {},
   "source": [
    "## 30. Add a column that contains the cumulative sum of the sales column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99533844-c519-46db-95a9-f46d6d2fac17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use a cumsum() method to find the cumulative sum of any column.\n",
    "df['cumulative_sales'] = df['total_sales'].cumsum()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc0a225-b7a5-4042-bae3-49ac4738e5b5",
   "metadata": {},
   "source": [
    "## 31. Extract all rows where the column name contains the substring \"John\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbcdb54-d59c-47b0-bc73-c4bd4b8ecbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use str.contains() method to extract row values as 'John'\n",
    "# Since we don't have any one under the substring 'John', it returned an empty dataframe\n",
    "sub_string_john = df[df['product'].str.contains('John')]\n",
    "sub_string_john"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84dd7f9-ec79-481e-ba3e-d11a6a782759",
   "metadata": {},
   "source": [
    "## 32. Filter rows where the column quantity is a multiple of 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422fc737-2f6a-404f-af9e-e5e90c53342e",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantity_of_five = df[df['quantity'] % 5 == 0]\n",
    "quantity_of_five"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ebe9b8-fc75-4e59-9fa5-361376d4ecc7",
   "metadata": {},
   "source": [
    "## 33. Count the number of rows in each category in the column product_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d336353b-add3-4d38-ab8f-2b68c602e99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use value_counts() to check the no.of rows in each category in product column.\n",
    "# value_counts() displays the values from largest to lowest/\n",
    "count_of_product_type = df['product'].value_counts()\n",
    "count_of_product_type\n",
    "# Below code also displays the no.of rows in each category. This displays the values in alphabetical order of category.\n",
    "#count_of_product_type = df.groupby('product').size()\n",
    "#count_of_product_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e64541-9f59-4d1f-9102-0d92ee2e9133",
   "metadata": {},
   "source": [
    "## 34. Find the correlation between numerical columns in a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637f4927-a9fe-464c-8cba-638b09fad704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation can found in dataframe using dataframe.corr().\n",
    "# Since we already have numerical_features, we put the dataframe around it and use .corr()\n",
    "corr_matrix = df[numerical_features].corr()\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0cb519-4941-4743-b730-bd0681f9a1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also display the correlation using heatmap from seaborn library.\n",
    "sns.heatmap(df[numerical_features].corr(), annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1069b6-0c92-44c1-a6b3-9a5f217e4912",
   "metadata": {},
   "source": [
    "## 35. Convert the order_date column to a datetime object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce6ac0d-0c01-4ba9-b052-de02e42f73cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before changing the datatype, let's check the dtypes first.\n",
    "col_types = df.dtypes.to_dict()\n",
    "col_types\n",
    "# We can also check the datatypes of all columns as per below\n",
    "# df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d91e3f2-0ce2-4394-939b-53f870c46126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will change the datatype of 'order_date' column from '<M8[ns]' to datetime\n",
    "type_change = pd.to_datetime(df['order_date'])\n",
    "df.info()\n",
    "# We can also below code to change it to datetime datatype. But this is not the best practice.\n",
    "#type_change = df['order_date'].astype('datetime64[ns]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2ab4ee-b2e1-43d3-96a8-86619db778a5",
   "metadata": {},
   "source": [
    "## 36. Slice the first 5 rows of the DataFrame and reset their index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c9fc69-368b-44dc-a0e5-0a103f14b39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice the first 5 rows of the DataFrame and reset their index\n",
    "sliced_dataframe = df.iloc[:5].reset_index(drop=True)\n",
    "sliced_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14937114-71de-49e3-9d32-fc96fa78c6a3",
   "metadata": {},
   "source": [
    "## 37. Multiply all values in the column quantity by 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fae66bd-6a40-46e8-82c5-4a0341a9278a",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiply_by_two = df['quantity'] * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7599cdf-ba6f-420b-ba78-c57804ea98b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiply_by_two"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b073820e-d26d-464f-be38-af73c5be139c",
   "metadata": {},
   "source": [
    "## 38. Check if any column in the DataFrame has duplicate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7645a331-d001-401c-8528-a0e53a357d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To just check the dataframe is duplicated or not, below code will work.\n",
    "# sum() will give the output in integer. If except '0' any number is displayed, it means the duplicates are there\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdbbd51-b10d-4ac4-819e-775663ea98ec",
   "metadata": {},
   "source": [
    "## 39. Export the DataFrame to an Excel file with the name data.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ff923d-1662-4d68-a5a6-ef5a6b073838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's export the dataframe to an excel.\n",
    "df.to_excel(r'E:\\Study\\Projects\\EDA\\data.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb69bde-d0b5-43e6-aad1-18f8f4893645",
   "metadata": {},
   "source": [
    "## 40. Remove all columns with more than 50% missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ffbae1-bc81-418d-a770-6941878018c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is the code for removing all columns greater than 50% of missing values.\n",
    "# Since we don't have any missing value greater than 50%, it returned an empty list.\n",
    "half_missing_values = [feature for feature in df.columns if df[feature].isnull().mean() > 0.50]\n",
    "half_missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c97c9d-215b-4c99-a920-a1ba64b0dd5b",
   "metadata": {},
   "source": [
    "## Moderate type of questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31cf50a-1ee0-4df4-b933-1b7cc0925727",
   "metadata": {},
   "source": [
    "## 1.Find the top 3 products with the highest sales in each region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca318548-aa2c-4006-a938-4bd5e9dccc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is the code for finding the top 3 products with highest sales in each region.\n",
    "# Let's group by 'region' and apply an arbitrary function for the top 3 product sales\n",
    "top_three_products = df.groupby('region').apply(lambda x: x.nlargest(3, 'total_sales').reset_index(drop=True))\n",
    "top_three_products\n",
    "# Below code also provides the same result. But we use sort_values()\n",
    "#df_sorted = df.sort_values(['region', 'total_sales'], ascending=[True, False])\n",
    "#final_value = df_sorted.groupby('region').head(3)\n",
    "#final_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd15e0e-56da-4faa-9ae4-c351290bdac7",
   "metadata": {},
   "source": [
    "## 2.Create a new column called sales_category that labels sales values as \"Low\", \"Medium\", or \"High\" based on thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831e325a-51b7-442f-82a9-948489148ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The expectation is 'total_sales' value from 0 to 3999 must be labelled as low\n",
    "# and from 3999 to 6999 must be labelled as medium\n",
    "# and above 7000 must be labelled as high.\n",
    "# To stamp the 'total_sales' values with some labels based on threshold, we use cut() from pandas.\n",
    "\n",
    "# bins - an integer values that segerate the entire 'sales_category' values based on label types.\n",
    "# labels - a string that stamps the 'sales_category' values based on bins\n",
    "\n",
    "bin_values = [0, 4000, 7000, 9000]\n",
    "category_labels = 'Low Medium High'.split()\n",
    "df['sales_category'] = pd.cut(df['total_sales'], bins = bin_values, labels = category_labels, right = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0e8ea4-aa60-4c19-a7c2-5ec22dcfaf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0dbc5c-2565-4d82-8ded-3f3df239cd8c",
   "metadata": {},
   "source": [
    "## 3.Fill missing values in the price column using the median of the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0333b35-751d-43d0-9af8-9de6fb65d746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we don't have price column in our dataframe, let's first create it.\n",
    "# We create the price column with some random numbers from 1 to 100 for the whole dataframe.\n",
    "df['price'] = np.random.randint(1, 100, len(df))\n",
    "# We now insert the NaN values randomly for the given size(9 rows will have NaN values)\n",
    "df.loc[np.random.choice(df.index, size=9, replace=False), 'price'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b37696-d8b7-430b-84b0-32fd3a914ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9905615d-e625-4ace-a5f3-02ead4d855eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fc16fe-2bf7-4259-8295-29d65db4890c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will first bring the price column to the right and then fill missing values with their median\n",
    "\n",
    "price_position = list(df.columns)\n",
    "price_position.insert(7, price_position.pop(-1))\n",
    "df=df[price_position]\n",
    "\n",
    "# Let's now fill missing values with median()\n",
    "#df['price'].fillna(df['price'].median(), inplace=True)\n",
    "df.fillna({'price': df['price'].median()}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa028830-5cee-482e-abca-88d0a526c0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2b0e3b-5a04-41d7-974c-6df5060b9089",
   "metadata": {},
   "source": [
    "## 4.Write a function to calculate the profit margin and apply it to the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5553150-b553-40cc-bb28-0133b6d79ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first see the profit margin formula\n",
    "# profit margin = (profit/sales) * 100\n",
    "def calculate_profit_margin(profit, sales):\n",
    "    return (profit/sales) * 100\n",
    "\n",
    "# Let's now apply the calculate_profit_margin function to the new column 'profit_margin'\n",
    "# Note, we use the arbitrary function lambda and use the same variable 'x' for passing the arguments.\n",
    "df['profit_margin'] = df.apply(lambda x:calculate_profit_margin(x['profit'], x['total_sales']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad508fe1-860b-4f5b-9ef8-7bbd43d74197",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942aaaa1-11c8-41db-84be-2a0af3949b52",
   "metadata": {},
   "source": [
    "## 5.Extract rows where the order_date falls within a specific date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce62d2d-6952-4f9c-a6bb-e21952e819b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use between() and specify the dates in string format. (i.e) in quotes.\n",
    "# Remember, do not use isin() and use range() for dates. As range() is for integers and cannot be used for datetime object.\n",
    "order_date_range = df[df['order_date'].between('2024-8-19' , '2024-12-17')]\n",
    "order_date_range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd244323-a31a-46f0-a37d-5420311f26ae",
   "metadata": {},
   "source": [
    "## 6.Write a script to normalize all numerical columns in the DataFrame to a scale of 0 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c9cbda-e8bf-4258-8265-bf51f77046be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To normalize the Dataframe between 0 and 1 we need to use MinMaxScaler class of preprocessing module from sklearn library\n",
    "# Note, StandardScaler() will scale between -1 and 0.\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "df[numerical_features] = scaler.fit_transform(df[numerical_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60aebec0-5d87-40e1-9fc0-045c65446e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[numerical_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621dcefc-e2fa-4dac-a09a-42e4c32434a8",
   "metadata": {},
   "source": [
    "## 7.Merge two DataFrames on multiple columns (id and region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd676590-7236-4007-8bd9-1097c693d2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we don't have region column in 'data' Dataframe, let's first create it.\n",
    "region_values = 'southern eastern western northern zonal divisional'.split()\n",
    "data['region'] = np.random.choice(region_values, size = len(data))\n",
    "\n",
    "# Now, let's merge both 'df' and 'data' Dataframe on multiple columns (id & region)\n",
    "# Since there are no common value in both Dataframe, the resultant will be an empty Dataframe.\n",
    "merged_df = pd.merge(df, data, on=['id', 'region'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d886aa47-e9d2-4ccd-b695-8879f5adfda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df will be empty since no value in common in both Dataframe\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c441644b-3764-4486-974f-0b13ba4d00d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5a459b-51a9-4086-a794-8e42eb28018e",
   "metadata": {},
   "source": [
    "## 8.Create a multi-index DataFrame and access specific rows using multi-level indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e50da3a-6a63-4aa2-904a-5bf2684c9732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi indexing is possible in pandas using 'pd.MultiIndex' class and 'from_tuples()'.\n",
    "# We need to pass a list of tuples and names (names of indexes)\n",
    "\n",
    "index_dataframe = pd.MultiIndex.from_tuples([('India', 'Tamil Nadu'),\n",
    "                                             ('India', 'Kerala'),\n",
    "                                             ('Canada', 'Toronto'),\n",
    "                                            ('Canada', 'Montreal'),\n",
    "                                            ('America', 'San Francisco'),\n",
    "                                            ('America', 'Chicago'),],names=('Country', 'State'))\n",
    "\n",
    "# Now we create a dataframe and pass the index argument as the variable name created above.\n",
    "# randint(20, 2980) denotes a random number starting from 20 to 2980 to be filled in the dataframe\n",
    "# size(6,4) denotes 6 rows and 4 columns.\n",
    "multi_index = pd.DataFrame(np.random.randint(20, 2980, size=(6,4)),\n",
    "                           index_dataframe,\n",
    "                           'Purchase Sales Profit Margin'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601930af-ad77-4c2c-97de-12a7bd153a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_index.head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d381e0-d150-4766-bb48-f19fd5b11d63",
   "metadata": {},
   "source": [
    "## 9.Create a histogram for the column quantity and find the most frequent bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c07878-fe1d-48d9-b8a8-394db3fd08b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can create histogram using plt.hist(), sns.histplot(), np.histogram(). But we are using plt.hist() here\n",
    "\n",
    "#plt.hist() returns three outputs. We unpack them using the variables 'counts, bin_edges, patches.\n",
    "\n",
    "#counts - It is an array that gives the frequency (count of values) of the each bin (here [2. 2. 3. 0. 2. 3. 3. 2. 2. 3.])\n",
    "\n",
    "#bin_edges - It is an array that gives the bin edges or the value of \n",
    "#each bin in x-axis (here [0.  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1. ])\n",
    "\n",
    "#patches - This is a list of patch objects. here(region xy=(2,0), width=19.8, height=2, angle=0)\n",
    "counts, bin_edges, patches = plt.hist(df['quantity'], bins=10, color='blue', edgecolor='black')\n",
    "\n",
    "#np.argmax() gives the indices of the value of the highest bin along the axis.(here 2 is the bin with highest frequency)\n",
    "max_bin_index = np.argmax(counts)\n",
    "\n",
    "#Now we need to get the starting point and end point of the largest bin with highest frequency\n",
    "most_frequent_bin = (bin_edges[max_bin_index], bin_edges[max_bin_index + 1])\n",
    "\n",
    "#plt.axvspan() Add a vertical span (rectangle) across the Axes. \n",
    "#plt.axvspan() highlights the most frequent bin in the histogram using a shaded region.\n",
    "plt.axvspan(most_frequent_bin[0], most_frequent_bin[1], color='red', alpha=0.3, label='most frequent bin')\n",
    "plt.xlabel('Quantity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Seaborn Histogram of Quantity')\n",
    "plt.legend()\n",
    "print(f\"Most Frequent Bin Range: {most_frequent_bin}, Count: {counts[max_bin_index]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1026093-5815-490a-a894-30d46fddee54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use try exception to throw the error\n",
    "#try:\n",
    "#    counts, bins, patches = plt.hist(df['quantity'], bins=10)\n",
    "#except Exception as e:\n",
    "#    print(f\"Unpacking Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57f0c09-e250-499c-803f-0faf8c065a5e",
   "metadata": {},
   "source": [
    "## 10.Extract rows where the column description has a specific word, ignoring case sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe226af0-92d9-474a-aeaf-3010432f7a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.loc[4, 'category']\n",
    "df[df['category'].str.contains('in', case=False, na=False)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1ae819-4172-45ab-85a3-3cbad304cddd",
   "metadata": {},
   "source": [
    "## 11.Write a script to calculate the percentage of missing values for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75378c8c-5be4-4a03-b7c5-9516ddf5c362",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_features = [feature for feature in df.columns if df[feature].isnull().sum() > 0]\n",
    "for fea in null_features:\n",
    "    print(f'{fea}: {np.round(df[fea].isnull().mean() * 100, 4)} % of missing values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e639deae-92b3-4ddd-b1f5-7b4467bdf84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_value_percent(df):\n",
    "    null_features = df.isnull().mean() * 100\n",
    "    missing_df = pd.DataFrame({'columns': df.columns, 'missing_features': null_features})\n",
    "    missing_df = missing_df[missing_df['missing_features'] > 0]\n",
    "    return missing_df.sort_values(by='missing_features', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6431f968-30f7-4d53-8bb2-679b4ac8ac18",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_value_percent(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5827c510-6d0d-49a1-a1dc-50fc7b39850a",
   "metadata": {},
   "source": [
    "## 12.Create a DataFrame showing the rank of sales within each region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8354a63-e7a5-4baf-b4d9-a44b40ceac38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.groupby('region').apply(lambda x:x.assign(sales_rank=x['total_sales'].rank(ascending=False, method='dense')))\n",
    "df['sales_rank'] = df.groupby('region')['total_sales'].rank(ascending=False, method='dense')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad78ebb-38aa-4be8-943e-d60476e1f317",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To list out all pandas Dataframe methods excluding attributes.\n",
    "# There are about 390 methods only excluding attributes in total.\n",
    "methods = [method for method in dir(df) if callable(getattr(df, method))]\n",
    "methods_df = pd.DataFrame({'Formattable_methods':methods})\n",
    "pd.set_option('display.max_rows', None)\n",
    "methods_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a769b3f7-fc9c-4e25-ba9c-b154b931ba4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To list out all pandas Dataframe methods including attributes.\n",
    "# There are about 526 methods including attributes and methods.\n",
    "df_methods = dir(df)\n",
    "listed_df = pd.DataFrame({'All Methods including attributes':df_methods})\n",
    "pd.set_option('display.max_rows', None)\n",
    "listed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03896380-8efb-4f7a-bf2c-3e711d80f1b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To check the structrure of methods like rank or to find the parameters.\n",
    "# This will help when intellisense not works sometime and difficult to find the parameters.\n",
    "help(pd.DataFrame.rank)\n",
    "# You can also give in below format.\n",
    "# help(df.rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fa39f1-0d9f-4b58-8007-61a74aa3dc33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Another way to check the structure of a method like rank.\n",
    "# This way works only when we are working in environments like jupyter notebook.\n",
    "pd.DataFrame.rank?\n",
    "# Below format also works\n",
    "#df.rank?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b006d5b8-9334-41f4-a95e-90c56d3b6c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to check the structure of methods like rank.\n",
    "# You need to import inspect module before using it.\n",
    "# Use signature method\n",
    "import inspect\n",
    "print(inspect.signature(pd.DataFrame.rank))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d693fc-0be2-49bc-a599-94a6f2820301",
   "metadata": {},
   "source": [
    "## 13.Find all rows where the sales column is equal to its maximum value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98775adc-e09f-4330-86b6-db4ef0e46845",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['total_sales']==df['total_sales'].max()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e13d731-5896-49c8-9dee-3103c969883e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to check the result of a function or an output is a single scalar value or pandas Series or DataFrame\n",
    "# First, We can use type()\n",
    "max_value = df['total_sales'].max()\n",
    "series_result = df['total_sales']\n",
    "dataframe_result = df[df['total_sales']==df['total_sales'].max()]\n",
    "print(type(max_value))\n",
    "print(type(series_result))\n",
    "print(type(dataframe_result))\n",
    "# If the output is '<class 'float'>' then it's a scalar value. Meaning, it is just a single number.\n",
    "# If the output is '<class 'pandas.core.series.Series'>' then it's a Series. It is a pandas series\n",
    "# If the output is '<class 'pandas.core.frame.DataFrame'>' then it's a DataFrame.\n",
    "# Second, we can use 'ndim' method. The output will be 0, 1 & 2. If 0-> scaler, 1-> series, 2-> DataFrame\n",
    "print(dataframe_result.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd002634-235c-4468-99a9-76dfae02c4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df['total_sales'].max())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2dbb8f00-e296-474a-a391-97c109dd853d",
   "metadata": {},
   "source": [
    "## 14.Create a rolling average for the column profit with a window size of 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6197b463-d126-4a15-afa5-cfc31c6484e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rolling means that the calculation (like sum, mean, etc.) will be performed on a moving window of 3 consecutive values.\n",
    "# The rolling calculation will be done based on a value provided in window parameter.\n",
    "df['rolling_profit_average'] = df['profit'].rolling(window=3).mean()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4d7c17-b274-4758-98e7-95854b76a54f",
   "metadata": {},
   "source": [
    "## 15.Split a column full_name into two columns: first_name and last_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627e977f-199d-413f-aab4-5ace372cfccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use 'data' DataFrame and not 'df' for this question.\n",
    "# Since we don't have any full_name column or other column to split, we first create in a data DataFrame.\n",
    "full_name = ['John Doe', 'Shahrukh Khan', 'Arnold Dan', 'Brown Dean']\n",
    "# We randomly fill the above names in the 'full_name' column (df['full_name'])\n",
    "data['full_name'] = np.random.choice(full_name, size=len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7efe4b8-a36d-4f49-a613-df5e6beeb542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now create 2 columns ('first_name' & 'last_name') to split the 'full_name'\n",
    "# fillna(\"\") -> not mandatory. But helps to fill NaN when the second name is not available. eg.'John' only\n",
    "# n=1 -> helps to split name by first space. eg. John Madonna Doe means, John as first name and Madonna Doe as second name.\n",
    "data[['first_name', 'last_name']] = data['full_name'].fillna(\"\").str.split(n=1, expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97336a4c-aaa1-4a2f-a748-faaa8409edca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef985e60-6599-4f9c-93ee-251fc87690b3",
   "metadata": {},
   "source": [
    "## 16.Create a bar plot of total sales for each region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98627b0-cda0-4b73-bae7-4a29a19bcaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a bar plot of total sales for each region using groupby()\n",
    "df.groupby('region')['total_sales'].sum().plot(kind='bar', color='blue', edgecolor='black')\n",
    "plt.title('Total sales of each region')\n",
    "plt.xlabel('Region')\n",
    "plt.ylabel('Total Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d533f65e-5e7d-43a6-983d-e741b110b40a",
   "metadata": {},
   "source": [
    "## 17.Calculate the z-score for the sales column and create a new column called z_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecdad24-1ea8-4237-873c-fc3d4cf289aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formula for z-score is (x-mu)/sigma.\n",
    "# where x is the individual 'total_sales' value. In our case (0.762919, 0.712614, 0.075173, 0.545764 etc..)\n",
    "# mu is the mean of all 'total_sales' value or the 'total_sales' column. We have a direct method in pandas as mean()\n",
    "# sigma is the standard deviation of 'total_sales' value or the 'total_sales' column. We have a direct method in pandas as std()\n",
    "mean_total_sales = df['total_sales'].mean()\n",
    "standard_deviation_total_sales = df['total_sales'].std()\n",
    "df['z-score'] = (df['total_sales'] - mean_total_sales)/standard_deviation_total_sales\n",
    "\n",
    "# We can also achieve this in 2 steps using scipy library\n",
    "# We have a readymade method called zscore in stats module of scipy library.\n",
    "#from scipy.stats import zscore\n",
    "#df['z_score_total_sales'] = zscore(df['total_sales'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f9a1b2-65de-44ef-acc4-dadb2f71500d",
   "metadata": {},
   "source": [
    "## 18.Group by region and find the percentage contribution of sales for each region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd35772a-d78c-4807-9eb9-253d4894e71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('region').apply(lambda x:x.assign(percent_contribution=(x['total_sales']/x['total_sales'].sum())*100))\n",
    "df[['region','total_sales']]\n",
    "# We can also achieve the above requirement using transform()\n",
    "#df.groupby('region')['total_sales'].transform(lambda x:(x/x.sum())*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282453d1-6113-474c-9aea-79a8d8dadee3",
   "metadata": {},
   "source": [
    "## 19.Write a function to check if a column has outliers and return the rows containing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab7753a-ca7e-4440-b7bb-6ddfe4de2036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a definition using the parameters df, column, method & threshold.\n",
    "# method-This argument helps the developer to give the flexibility of type of outlier detection such as z-score or IQR.\n",
    "def detect_outliers(df, column, method='iqr', threshold=1.5):\n",
    "    try:\n",
    "        #column = df.columns\n",
    "        if method == 'zscore':\n",
    "            mu_mean = df[column].mean()\n",
    "            std_dev = df[column].std()\n",
    "            return df[((df[column]-mu_mean) / std_dev).abs() > threshold]\n",
    "        elif method == 'iqr':\n",
    "            q1 = df[column].quantile(0.25)\n",
    "            q3 = df[column].quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = q1 - threshold * iqr\n",
    "            upper_bound = q3 + threshold * iqr\n",
    "            return df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "        else:\n",
    "            raise ValueError ('Method must be either IQR or Z-Score')\n",
    "    except Exception as e:\n",
    "        print(f\"The error is {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae49986-5f92-4066-8701-92ee6f50b1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we don't have a huge threshold for 'total_sales' column we pass it as 0.3 to get the result.\n",
    "detect_outliers(df, 'total_sales', method='iqr', threshold=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14389d23-e34e-48f1-8170-369e428a5bd9",
   "metadata": {},
   "source": [
    "## 20.Convert a column with currency values (e.g., \"$100\") into numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d449adda-ead9-4652-b593-b2397235db03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we don't have the currency values, let's first create it.\n",
    "df['currency'] = np.random.randint(20, 800, size=len(df))\n",
    "# Now we shall add '$' as currency prefix to all 'currency' values. This can be achieved in 3 ways.\n",
    "df['currency'] = df['currency'].apply(lambda x:f\"${x:.2f}\") # first method, using apply()\n",
    "#df['currency'] = df['currency'].map(\"${:,.2f}\".format) # using map()\n",
    "#df.style.format({\"currency\":\"${:,.2f}\"}) # using style.format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d22f1d7-aefb-4e90-9b06-72661160cd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's convert the currency column into numerical column by removing the '$' symbol\n",
    "# The regex pattern is used in replace() and removes the $ sign\n",
    "# Finally converting the Object type to float datatype.\n",
    "df['currency'] = df['currency'].str.replace(r'[^\\d.]', \"\", regex=True).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6f9c3d-d68a-4bd3-8f35-2593bb2feb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d34c165-f35c-43c9-99c5-398988130aa7",
   "metadata": {},
   "source": [
    "## 21. Extract all rows where the column id has duplicate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5806dba-f411-4378-ab3b-a60c8bb25b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since there is no duplicate in 'ID' column, we get an empty row as the result.\n",
    "df[df['id'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8e4350-1bed-4f2b-9936-cb0aac5fa7df",
   "metadata": {},
   "source": [
    "## 22.Create a stacked bar chart for sales across region and category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bd76ed-2e4c-4de7-a065-92ddf9281a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['region', 'category'])['total_sales'].sum().unstack().plot(kind='bar', stacked=True)\n",
    "plt.title('Stacked Bar Chart of Sales by Region and Category')\n",
    "plt.xlabel('Region')\n",
    "plt.ylabel('Total_Sales')\n",
    "plt.legend(title='Category')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c64975-f50b-4090-87ea-9221fdd148ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Another method to Create a stacked bar chart for sales across region and category\n",
    "\n",
    "# Step 1: Pivot the Data to get stacked values\n",
    "df_pivot = df.pivot(index='region', columns='category', values='total_sales')\n",
    "\n",
    "# Step 2: Set up plot\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Define colors for categories\n",
    "colors = sns.color_palette(\"Set2\", len(df_pivot.columns))\n",
    "\n",
    "# Step 3: Plot each category stacked on the previous one\n",
    "bottom = None\n",
    "for idx, category in enumerate(df_pivot.columns):\n",
    "    sns.barplot(x=df_pivot.index, y=df_pivot[category], color=colors[idx], ax=ax, label=category, bottom=bottom)\n",
    "    bottom = df_pivot[category] if bottom is None else bottom + df_pivot[category]\n",
    "\n",
    "# Step 4: Customize the chart\n",
    "plt.title('Stacked Bar Chart of Sales by Region and Category')\n",
    "plt.xlabel('Region')\n",
    "plt.ylabel('Total Sales')\n",
    "plt.legend(title='Category')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "# Step 5: Show the Plot\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd34ca73-c746-40f9-9a58-2a9b878a66a1",
   "metadata": {},
   "source": [
    "## 23.Write a script to identify and remove outliers from the column quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1588ed5-7168-4993-a4bb-4894d4d23f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use zscore method to identify outliers of 'quantity' column.\n",
    "# The default threshold value for zscore is 3\n",
    "# We use zscore formula(x-mu/sigma) to calculate\n",
    "# |z| > 3 (absolute values of z-score) greater than 3 are outliers.\n",
    "mean_quantity = df['quantity'].mean()\n",
    "std_quantity = df['quantity'].std()\n",
    "df['quantity'] = df[((df['quantity']-mean_quantity)/std_quantity).abs() > 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea9f54f-bb41-4906-a81b-834b366e10f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
